{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15962648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos las librerías\n",
    "import os\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2005686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los archivos\n",
    "train_dataset = pd.read_csv(\"../files/input/train_data.csv.zip\", index_col=False)\n",
    "test_dataset = pd.read_csv(\"../files/input/test_data.csv.zip\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdb436e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1.\n",
    "# - Renombre la columna \"default payment next month\" a \"default\".\n",
    "train_dataset.rename(columns={\"default payment next month\": \"default\"}, inplace=True)\n",
    "test_dataset.rename(columns={\"default payment next month\": \"default\"}, inplace=True)\n",
    "\n",
    "# # - Remueva la columna \"ID\".\n",
    "train_dataset.drop(columns={\"ID\"}, inplace=True)\n",
    "test_dataset.drop(columns={\"ID\"}, inplace=True)\n",
    "\n",
    "# # - Elimine los registros con informacion no disponible.\n",
    "test_dataset.dropna()\n",
    "train_dataset.dropna()\n",
    "\n",
    "# Esta sería la otra forma de eliminar los NA\n",
    "# train_dataset = train_dataset.loc[train_dataset[\"MARRIAGE\"] != 0]\n",
    "# train_dataset = train_dataset.loc[train_dataset[\"EDUCATION\"] != 0]\n",
    "# test_dataset = test_dataset.loc[test_dataset[\"MARRIAGE\"] != 0]\n",
    "# test_dataset = test_dataset.loc[test_dataset[\"EDUCATION\"] != 0]\n",
    "\n",
    "\n",
    "# # - Para la columna EDUCATION, valores > 4 indican niveles superiores\n",
    "# #   de educación, agrupe estos valores en la categoría \"others\".\n",
    "train_dataset[\"EDUCATION\"] = train_dataset[\"EDUCATION\"].apply(lambda x: 4 if x > 4 else x)\n",
    "test_dataset[\"EDUCATION\"] = train_dataset[\"EDUCATION\"].apply(lambda x: 4 if x > 4 else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50c0a3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 2.\n",
    "# Divida los datasets en x_train, y_train, x_test, y_test.\n",
    "\n",
    "\n",
    "x_train = train_dataset.drop(columns=[\"default\"])\n",
    "y_train = train_dataset[\"default\"]\n",
    "x_test = test_dataset.drop(columns=[\"default\"])\n",
    "y_test = test_dataset[\"default\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15e78b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3.\n",
    "# Cree un pipeline para el modelo de clasificación. Este pipeline debe\n",
    "# contener las siguientes capas:\n",
    "# - Transforma las variables categoricas usando el método\n",
    "#   one-hot-encoding.\n",
    "# - Ajusta un modelo de bosques aleatorios (rando forest).\n",
    "#\n",
    "\n",
    "categorical_features = [\"EDUCATION\", \"MARRIAGE\", \"SEX\"]\n",
    "\n",
    "transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"ohe\", OneHotEncoder(), categorical_features),\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"transformer\", transformer),\n",
    "    (\"rf\", RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "366d37f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mejores parámetros encontrados:\n",
      "{'rf__max_depth': None, 'rf__max_features': 25, 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 2, 'rf__n_estimators': 200}\n",
      "Mejor f1 ponderado (Validación cruzada): 0.6583434673796368\n"
     ]
    }
   ],
   "source": [
    "# Paso 4.\n",
    "# Optimice los hiperparametros del pipeline usando validación cruzada.\n",
    "# Use 10 splits para la validación cruzada. Use la función de precision\n",
    "# balanceada para medir la precisión del modelo.\n",
    "\n",
    "param_grid = {\n",
    "\"rf__n_estimators\": [100, 200],\n",
    "\"rf__max_depth\": [5, 10, None],\n",
    "\"rf__min_samples_split\": [2, 5],\n",
    "'rf__min_samples_leaf': [1, 2], \n",
    "\"rf__max_features\": [25],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv = 10,\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    n_jobs=-1,        \n",
    ")\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nMejores parámetros encontrados:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"Mejor f1 ponderado (Validación cruzada):\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07614ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 5.\n",
    "# Guarde el modelo (comprimido con gzip) como \"files/models/model.pkl.gz\".\n",
    "# Recuerde que es posible guardar el modelo comprimido usanzo la libreria gzip.\n",
    "\n",
    "def save_estimator(estimator):\n",
    "    models_path = \"files/models\"\n",
    "    os.makedirs(models_path, exist_ok=True)\n",
    "\n",
    "    model_file = os.path.join(models_path, \"model.pkl.gz\")\n",
    "\n",
    "    with gzip.open(model_file, \"wb\") as file:\n",
    "        pickle.dump(estimator, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "be26166a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Métricas (train) ===\n",
      "Precisión:            0.998\n",
      "Exactitud balanceada: 0.978\n",
      "Sensibilidad (Recall):0.956\n",
      "F1 Score:             0.977\n",
      "\n",
      "=== Métricas (test) ===\n",
      "Precisión:            0.645\n",
      "Exactitud balanceada: 0.672\n",
      "Sensibilidad (Recall):0.405\n",
      "F1 Score:             0.498\n",
      "\n",
      "✅ Métricas guardadas en: files/output\\metrics.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "\n",
    "# Paso 6: Calcular métricas\n",
    "def calc_metrics(model, x_train, y_train, x_test, y_test):\n",
    "    y_train_pred = model.predict(x_train)\n",
    "    y_test_pred = model.predict(x_test)\n",
    "\n",
    "    metrics = [\n",
    "        {\n",
    "            'dataset': 'train',\n",
    "            'precision': precision_score(y_train, y_train_pred, zero_division=0),\n",
    "            'balanced_accuracy': balanced_accuracy_score(y_train, y_train_pred),\n",
    "            'recall': recall_score(y_train, y_train_pred, zero_division=0),\n",
    "            'f1_score': f1_score(y_train, y_train_pred, zero_division=0)\n",
    "        },\n",
    "        {\n",
    "            'dataset': 'test',\n",
    "            'precision': precision_score(y_test, y_test_pred, zero_division=0),\n",
    "            'balanced_accuracy': balanced_accuracy_score(y_test, y_test_pred),\n",
    "            'recall': recall_score(y_test, y_test_pred, zero_division=0),\n",
    "            'f1_score': f1_score(y_test, y_test_pred, zero_division=0)\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Guardar las métricas en JSONL\n",
    "def save_metrics(metrics):\n",
    "    output_path = \"files/output\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    metrics_file = os.path.join(output_path, \"metrics.json\")\n",
    "\n",
    "    with open(metrics_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for metric in metrics:\n",
    "            json.dump(metric, f)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    print(\"✅ Métricas guardadas en:\", metrics_file)\n",
    "\n",
    "\n",
    "# 1. Crear pipeline\n",
    "pipeline = make_pipeline()\n",
    "\n",
    "# 2. Hacer Grid Search\n",
    "grid_search = make_grid_search(pipeline, x_train, y_train)\n",
    "\n",
    "# 3. Guardar el mejor modelo entrenado\n",
    "model = grid_search.best_estimator_\n",
    "\n",
    "# 4. Calcular métricas\n",
    "results = calc_metrics(model, x_train, y_train, x_test, y_test)\n",
    "\n",
    "# 5. Mostrar métricas\n",
    "for res in results:\n",
    "    print(f\"=== Métricas ({res['dataset']}) ===\")\n",
    "    print(f\"Precisión:            {res['precision']:.3f}\")\n",
    "    print(f\"Exactitud balanceada: {res['balanced_accuracy']:.3f}\")\n",
    "    print(f\"Sensibilidad (Recall):{res['recall']:.3f}\")\n",
    "    print(f\"F1 Score:             {res['f1_score']:.3f}\")\n",
    "    print()\n",
    "\n",
    "# 6. Guardar métricas en JSON\n",
    "save_metrics(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b40e20c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Matriz de confusión (train) ===\n",
      "        Pred_0   Pred_1\n",
      "True_0    16266        7\n",
      "True_1    208        4519\n",
      "\n",
      "=== Matriz de confusión (test) ===\n",
      "        Pred_0   Pred_1\n",
      "True_0    6666        425\n",
      "True_1    1136        773\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def matriz_confusion_dict(model, x_train, y_train, x_test, y_test):\n",
    "    # Predicciones\n",
    "    y_train_pred = model.predict(x_train)\n",
    "    y_test_pred = model.predict(x_test)\n",
    "\n",
    "    # Matrices de confusión\n",
    "    cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "    cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "    # Estructura en diccionario\n",
    "    matriz_confusion = [\n",
    "        {\n",
    "            'type': 'cm_matrix',\n",
    "            'dataset': 'train',\n",
    "            'true_0': {'predicted_0': int(cm_train[0, 0]), 'predicted_1': int(cm_train[0, 1])},\n",
    "            'true_1': {'predicted_0': int(cm_train[1, 0]), 'predicted_1': int(cm_train[1, 1])}\n",
    "        },\n",
    "        {\n",
    "            'type': 'cm_matrix',\n",
    "            'dataset': 'test',\n",
    "            'true_0': {'predicted_0': int(cm_test[0, 0]), 'predicted_1': int(cm_test[0, 1])},\n",
    "            'true_1': {'predicted_0': int(cm_test[1, 0]), 'predicted_1': int(cm_test[1, 1])}\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return matriz_confusion\n",
    "\n",
    "cm_results = matriz_confusion_dict(model, x_train, y_train, x_test, y_test)\n",
    "\n",
    "for cm in cm_results:\n",
    "    print(f\"=== Matriz de confusión ({cm['dataset']}) ===\")\n",
    "    print(f\"        Pred_0   Pred_1\")\n",
    "    print(f\"True_0    {cm['true_0']['predicted_0']}        {cm['true_0']['predicted_1']}\")\n",
    "    print(f\"True_1    {cm['true_1']['predicted_0']}        {cm['true_1']['predicted_1']}\")\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
